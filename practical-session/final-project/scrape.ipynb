{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bda5f93",
   "metadata": {},
   "source": [
    "# Web Scraping Song Data from sendthesong.xyz with Selenium\n",
    "\n",
    "This notebook demonstrates how to scrape song data from https://sendthesong.xyz/browse using Selenium for dynamic content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecd28011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Memulai proses scraping untuk query: 'adit' dengan target 300 data.\n",
      "[*] Mengambil data halaman 1 (0/300)...\n",
      "[*] Mengambil data halaman 2 (15/300)...\n",
      "[*] Mengambil data halaman 2 (15/300)...\n",
      "[*] Mengambil data halaman 3 (30/300)...\n",
      "[*] Mengambil data halaman 3 (30/300)...\n",
      "[*] Mengambil data halaman 4 (45/300)...\n",
      "[*] Mengambil data halaman 4 (45/300)...\n",
      "[*] Mengambil data halaman 5 (60/300)...\n",
      "[*] Mengambil data halaman 5 (60/300)...\n",
      "[*] Mengambil data halaman 6 (75/300)...\n",
      "[*] Mengambil data halaman 6 (75/300)...\n",
      "[*] Mengambil data halaman 7 (90/300)...\n",
      "[*] Mengambil data halaman 7 (90/300)...\n",
      "[*] Mengambil data halaman 8 (105/300)...\n",
      "[*] Mengambil data halaman 8 (105/300)...\n",
      "[*] Mengambil data halaman 9 (120/300)...\n",
      "[*] Mengambil data halaman 9 (120/300)...\n",
      "[*] Mengambil data halaman 10 (135/300)...\n",
      "[*] Mengambil data halaman 10 (135/300)...\n",
      "[*] Mengambil data halaman 11 (150/300)...\n",
      "[*] Mengambil data halaman 11 (150/300)...\n",
      "[*] Mengambil data halaman 12 (165/300)...\n",
      "[*] Mengambil data halaman 12 (165/300)...\n",
      "[*] Mengambil data halaman 13 (180/300)...\n",
      "[*] Mengambil data halaman 13 (180/300)...\n",
      "[*] Mengambil data halaman 14 (195/300)...\n",
      "[*] Mengambil data halaman 14 (195/300)...\n",
      "[*] Mengambil data halaman 15 (210/300)...\n",
      "[*] Mengambil data halaman 15 (210/300)...\n",
      "[*] Mengambil data halaman 16 (225/300)...\n",
      "[*] Mengambil data halaman 16 (225/300)...\n",
      "[*] Mengambil data halaman 17 (240/300)...\n",
      "[*] Mengambil data halaman 17 (240/300)...\n",
      "[*] Mengambil data halaman 18 (255/300)...\n",
      "[*] Mengambil data halaman 18 (255/300)...\n",
      "[*] Mengambil data halaman 19 (270/300)...\n",
      "[*] Mengambil data halaman 19 (270/300)...\n",
      "[*] Mengambil data halaman 20 (285/300)...\n",
      "[*] Mengambil data halaman 20 (285/300)...\n",
      "\n",
      "[+] Total data berhasil dikumpulkan: 300\n",
      "\n",
      "[SUCCESS] Data telah berhasil diekspor ke file: 'hasil_scraping_adit.csv'\n",
      "\n",
      "[+] Total data berhasil dikumpulkan: 300\n",
      "\n",
      "[SUCCESS] Data telah berhasil diekspor ke file: 'hasil_scraping_adit.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- KONFIGURASI ---\n",
    "API_URL = \"https://api.sendthesong.xyz/api/posts\"\n",
    "SEARCH_QUERY = 'adit'  # Ganti dengan query pencarianmu\n",
    "TARGET_DATA_COUNT = 300\n",
    "LIMIT_PER_PAGE = 50  # Mengambil 50 data per request agar lebih efisien\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Referer': 'https://sendthesong.xyz/' \n",
    "}\n",
    "\n",
    "# --- PROSES SCRAPING ---\n",
    "all_songs_data = []\n",
    "current_page = 1\n",
    "\n",
    "print(f\"[*] Memulai proses scraping untuk query: '{SEARCH_QUERY}' dengan target {TARGET_DATA_COUNT} data.\")\n",
    "\n",
    "while len(all_songs_data) < TARGET_DATA_COUNT:\n",
    "    params = {\n",
    "        'q': SEARCH_QUERY,\n",
    "        'page': current_page,\n",
    "        'limit': LIMIT_PER_PAGE\n",
    "    }\n",
    "\n",
    "    print(f\"[*] Mengambil data halaman {current_page} ({len(all_songs_data)}/{TARGET_DATA_COUNT})...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(API_URL, params=params, headers=HEADERS)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # ASUMSI: List lagu ada di dalam key 'data'. \n",
    "            # SESUAIKAN NAMA KEY INI ('data') JIKA BERBEDA DENGAN STRUKTUR JSON-MU.\n",
    "            # Contoh lain mungkin: data['posts'], data['results'], dll.\n",
    "            new_songs = data.get('data', []) \n",
    "            \n",
    "            if not new_songs:\n",
    "                print(\"[!] Halaman tidak berisi data baru atau sudah mencapai halaman terakhir. Menghentikan proses.\")\n",
    "                break\n",
    "            \n",
    "            all_songs_data.extend(new_songs)\n",
    "            current_page += 1\n",
    "\n",
    "            # Beri jeda 1 detik antar request agar tidak membebani server (good practice)\n",
    "            time.sleep(1) \n",
    "            \n",
    "        else:\n",
    "            print(f\"[!] Gagal mengambil data di halaman {current_page}. Status Code: {response.status_code}. Menghentikan proses.\")\n",
    "            break\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[!] Terjadi error koneksi: {e}. Menghentikan proses.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n[+] Total data berhasil dikumpulkan: {len(all_songs_data)}\")\n",
    "\n",
    "# --- PROSES KONVERSI & EXPORT KE CSV ---\n",
    "if all_songs_data:\n",
    "    try:\n",
    "        # Mengubah list of dictionaries menjadi Pandas DataFrame\n",
    "        df = pd.DataFrame(all_songs_data)\n",
    "        \n",
    "        # Menentukan nama file output\n",
    "        output_filename = f'hasil_scraping_{SEARCH_QUERY.replace(\" \", \"_\")}.csv'\n",
    "        \n",
    "        # Menyimpan DataFrame ke file CSV\n",
    "        # index=False agar nomor index dari DataFrame tidak ikut ditulis ke file\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n[SUCCESS] Data telah berhasil diekspor ke file: '{output_filename}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[!] Terjadi error saat mengonversi ke CSV: {e}\")\n",
    "else:\n",
    "    print(\"\\n[!] Tidak ada data untuk diekspor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6acf2",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- Increased the wait time for dynamic content to load (20 seconds).\n",
    "- Added logs to print the page source and count of song cards for debugging.\n",
    "- If the data still does not appear, inspect the website's structure for changes or use browser developer tools to debug.\n",
    "- Added a check to verify if song cards are found. If not, the page source is logged for debugging.\n",
    "- Ensure the CSS selector matches the website's structure. Use browser developer tools to inspect elements.\n",
    "- If the issue persists, the website may have additional dynamic loading mechanisms.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
